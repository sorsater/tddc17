\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{datetime2}
\usepackage{parskip}
\usepackage{lipsum}

\pagestyle{fancy}
\fancyhf{}

\lhead{eriro331, micso554}
\rhead{\today} % yyyy-mm-dd
\setlength{\headheight}{15pt}

\cfoot{\thepage}

\begin{document}

\begin{center}
    \Huge
    \textbf{TDDC17 - AI}

    \vspace{0.3cm}
    \Large
    Erik Rönmark
    Michael Sörsäter
    
    \vspace{0.7cm}
    \textbf{Lab5 report}
\end{center}

\section{Part II - angle}
\textbf{Describe your choices of state and reward functions, and describe in your own words the purpose of the different components in the Q-learning update that you implemented.}

We chose to have intervals for the state/reward functions by using the method discretize. In total we had 5 states(name - reward):

\begin{itemize}
\item ``Up'' - 100
\item``Left\_Light'' - 45
\item``Right\_Light'' - 45
\item``Left\_Strong'' - 20
\item``Right\_Strong'' - 20
\end{itemize}

The different components of Q-learning are:
\begin{itemize}
\item The Q-value of the current state
\item Alpha is the learning rate that decreases over time. The learning rate is based on how many times the specific state-action combination have been tried.
\item The reward for the current state-action.
\item The discount factor depends if you want a short-sighted or long-sighted solution. In our system, the discount factor is 0.95 so it is long-sighted, this is because we want it to value more information that helps in the long run. 
\item Tries to find the best action for the current state.
\item The old value.
\end{itemize}						
													
\textbf{Try turning off exploration from the start before learning. What tends to happen? Explain why this happens in your report.}

It will get stuck in a local optimum and will not perform well. This is because it doesn't test new things and can't get out of the local optimum.

\section {Part III - hover}
\textbf{Description of the implementation}

We take in to consideration the angle, vy and vx.

Our solution is based on that if we can keep a straight angle and a low vertical velocity, the rocket will only drift very little.

The most important parameters are the angle and the vertical speed. We use the discretize function to split the values in to 20 states each and give them rewards linearly.

We do this on two levels, a raw tuning and a fine tuning. This is to limit the number of states but still perform well. 

The horizontal velocity is only used to reward if the rocket have a very low velocity. 

\end{document}